<!DOCTYPE html><html lang="en" mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.7.2"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛)" /><meta name="author" content="Bae Soomin" /><meta property="og:locale" content="en_US" /><meta name="description" content="[DAY 7] Gradient Descent - Hard" /><meta property="og:description" content="[DAY 7] Gradient Descent - Hard" /><link rel="canonical" href="https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/" /><meta property="og:url" content="https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/" /><meta property="og:site_name" content="Always Awake Sally" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-26T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛)" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Bae Soomin" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Bae Soomin"},"description":"[DAY 7] Gradient Descent - Hard","url":"https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/","@type":"BlogPosting","headline":"[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛)","dateModified":"2021-03-30T03:40:57+09:00","datePublished":"2021-01-26T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/"},"@context":"https://schema.org"}</script><title>[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛) | Always Awake Sally</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/my_image/Sally_wallpaper.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Always Awake Sally</a></div><div class="site-subtitle font-italic">24시간이 모자라</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tabs/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tabs/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/tabs/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tabs/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/bsm8734" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bsm8734','naver.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛)</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 26, 2021, 12:00 AM +0900" > Jan 26 <i class="unloaded">2021-01-26T00:00:00+09:00</i> </span> by <span class="author"> Bae Soomin </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Mar 30, 2021, 3:40 AM +0900" > Mar 30 <i class="unloaded">2021-03-30T03:40:57+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2315 words">12 min</span></div></div><div class="post-content"><h2 id="day-7-gradient-descent---hard"><strong>[DAY 7] Gradient Descent - Hard</strong></h2><hr /><h3 id="선형회귀-복습"><strong>선형회귀 복습</strong></h3><ul><li>선형회귀: n개의 데이터로 이루어진 상황에서 데이터를 가장 잘 표현하는 모델을 찾는 것<ul><li>무어-펜로즈: 정답에 근사한 값을 사용해서, 선형모델의 계수를 쉽게 찾을 수 있음<li>경사하강법<li>sklearn linear regression 라이브러리<li><code class="language-plaintext highlighter-rouge">np.linalg.pinv</code>를 이용하여 <strong>선형모델</strong>(linear model)로 해석하는 <strong>선형회귀식</strong>을 찾을 수 있음</ul></ul><h4 id="선형모델회귀식-찾기"><strong>선형모델(회귀식) 찾기</strong></h4><ul><li>선형모델의 경우, 역행렬을 이용하여 회귀분석이 가능<li>역행렬을 이용하지 않고, 경사하강법 이용 가능 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-02-45-33.png" alt="get linear model" /></ul><h4 id="경사하강법으로-선형회귀-계수-구하기"><strong>경사하강법으로 선형회귀 계수 구하기</strong></h4><ul><li>선형회귀의 목적식: $\parallel y-X_\beta \parallel_2$ (= L2-norm)<li>선형회귀의 목적식을 <strong>최소화</strong>하는 $\beta$를 찾아야하므로 다음과 같은 그레디언트 벡터를 구하는 것이 목적임</ul><hr /><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-19-48.png" alt="p4" /></p><ul><li>$y$: 정답<li><p>$X_\beta$: 행렬($X$), 벡터($\beta$)</p><li>설명<ul><li>(두 벡터의 차이인) L2-norm을 최소화하는 $\beta$ 찾기<li><strong>$\beta$ 최소화</strong>: 목적식을 $\beta$로 미분하고 주어진 $\beta$에서 미분값을 빼면 최소값을 찾을 수 있음<li>$\lVert y-X_\beta \rVert_2$ 대신 ${\lVert y-X_\beta \rVert_2}^2$를 최소화 해도 됨 <em>(목적식 대신 목적식의 제곱)</em></ul></ul><hr /><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-21-58.png" alt="p5" /></p><ul><li>${1 \over n} \sum_{i=1}^n$<ul><li>$1 \over 2$ : 평균을 구하기 위해 사용<li>$\sum_{i=1}^n$ : N개의 데이터를 가지고 계산한다는 의미</ul><li>$\partial_{\beta_k} \lVert y-X_\beta \rVert_2$ : 주어진 $\beta$ 벡터 내에서 $k$번째 계수에 해당하는 $\beta_k$를 사용하여 목적식을 편미분한 것<li>$\sum_{j=1}^d X_{ij}$ : $N$개의 데이터를 가지고 계산<li>L2-norm이므로 제곱합의 제곱근 사용</ul><hr /><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-22-57.png" alt="p6" /></p><ul><li>$\partial_{\beta_k} \lVert y-X_\beta \rVert_2$의 목적식: <strong>$\lVert y-X_\beta \rVert_2$</strong><li><p>$X_k^T$ : 행렬$X$의 $k$번째 열(column)벡터를 전치시킨 것</p><li>설명<ul><li>$k$번째 계수, 벡터 $k$에 대한 목적식의 편미분<li>➡ 각 요소에 대한 편미분이며, $\beta_1$ ~ $\beta_n$까지의 gradient vector 구할 수 있음</ul></ul><hr /><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-25-56.png" alt="p9-1" /></p><ul><li>$X_\beta$(선형모델)를 계수 $\beta$에 대해 미분한 결과인 $X^T$만 곱해진다는 것을 의미<ul><li>➡ 이러한 점은 D차원이나 1차원이나 동일함</ul></ul><hr /><ul><li><p><strong>목적식을 최소화하는 $\beta$를 구하는 경사하강법 알고리즘</strong> <br /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-44-16.png" alt="p9-2" /> 위의 식을 풀면 다음과 같음 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-31-42.png" alt="p10" /></p><ul><li>t를 반복적으로 계산해보면 목적식을 최적화하는 $\beta$를 구할 수 있음<li>이전과정에 비해, 다변수인 것을 제외하면 모든 요소가 같음<li>$\beta^{(t)}$: t번째 단계에서의 coefficient<li>$\lambda$ : 학습률 - 수렴속도 지정<li>$\triangledown_\beta$ : gradient vector(미분값)<li>$\lambda \triangledown_\beta \lVert y-X\beta^(t) \rVert$ : gradient factor</ul></ul><blockquote><p>선형회귀의 목적식은 L2-norm 과 같고, 경사하강법에서 L2-norm 대신 L2-norm의 제곱을 사용하는 것도 가능함(식이 간단해짐!) ➡ 둘은 동일한 결과를 가져옴</p></blockquote><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-04-55-08.png" alt="compare made by sm" /></p><hr /><h4 id="경사하강법-기반-선형회귀-알고리즘"><strong>경사하강법 기반 선형회귀 알고리즘</strong></h4><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># Input: X, y, lr, T
# Output: beta
</span><span class="s">"""
norm: L2-norm을 계산하는 함수
lr: 학습률
T: 학습횟수
"""</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>        <span class="c1"># eps로 사용가능, 지정된 시간의 종료조건
</span>    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="o">-</span> <span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">error</span> <span class="c1"># @: 행렬곱
</span>    <span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
</pre></table></code></div></div><ul><li>종료 조건을 일정 학습횟수로 변경한 점만 제외하면 앞에서 배운 경사하강법 알고리즘과 같음<li><code class="language-plaintext highlighter-rouge">error</code>, <code class="language-plaintext highlighter-rouge">grad</code>, <code class="language-plaintext highlighter-rouge">beta</code>를 구하는 과정 : $\triangledown_\beta \lVert y-X_\beta \rVert_2^2$ 항을 계산해서 $\beta$를 업데이트하는 과정<ul><li>$\beta$: 주어진 목적식을 최소화하는 선형모델의 계수<li>무어-펠로스 역행렬 없이 계수찾기 가능</ul><li>경사하강법 알고리즘을 사용하면 역행렬을 이용하지 않고 회귀계수를 계산할 수 있음<li>경사하강법 알고리즘에서 <strong>학습률과 학습횟수</strong>는 중요한 hyperparameter<ul><li>학습률(<code class="language-plaintext highlighter-rouge">lr</code>)이 너무 작을 경우 ➡ 수렴이 늦어짐<li>학습률이 너무 클 경우 ➡ 불안정한 gradient 움직임이 발생함</ul></ul><h4 id="convex-function-볼록함수"><strong>Convex Function (볼록함수)</strong></h4><ul><li>이론적으로, 경사하강법은 <u>미분가능하고 볼록한 함수에 대하여, 적절한 학습률과 학습횟수를 선택했을 때, 수렴이 보장되어 있음</u><li>볼록한 함수는 그레디언트 벡터가 항상 최소점을 향함<li>특히 선형회귀의 경우, 목적식 $\lVert y-X_\beta \rVert_2$은 회귀계수 $\beta$에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됨 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-05-18-51.png" alt="p17" /><li>그러나 비선형회귀 문제의 경우, 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않음<ul><li>특히 딥러닝을 사용하는 경우, 목적식은 대부분 볼록함수가 아님(non-convex function, 볼록성 보장X) <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-05-19-19.png" alt="p18" /></ul></ul><hr /><h3 id="확률적-경사하강법"><strong>확률적 경사하강법</strong></h3><ul><li>(SGD)stochastic gradient descent<li>모든 데이터를 사용하여 업데이트하는 대신,<u>데이터 한개 혹은 일부를 활용</u>하여 업데이트 진행<ul><li>데이터를 일부 활용하는 것을 <strong>mini batch</strong>라고 함<li>요즘의 SGD는 mini batch SGD를 의미하곤 함</ul><li>볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화 가능<ul><li>언제나 성립하는 것은 아니지만, 딥러닝의 경우, SGD가 경사하강법보다 실증적으로 더 낫다고 검증되었음</ul></ul><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-05-21-34.png" alt="p19" /></p><ul><li>$E$: Expectation, 무한시행 시에서의 평균값<li>$\widehat{\triangledown_\theta\mathcal{L}}(\theta^{t})$ : stocastic 추정치<li>$\triangledown_\theta\mathcal{L}$ : gradient<li>SGD는 데이터의 일부를 가지고 파라미터를 업데이트하기 때문에 <strong>연산자원을 좀 더 효율적으로 활용</strong>하여 파라미터 최적화 가능 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-05-26-59.png" alt="p20" /><ul><li>전체데이터 $(X, y)$를쓰지않고미니배치$(X_{(b)}, y_{(b)})$를 써서 업데이트하므로 연산량이 $b \over n$로 감소함<ul><li>$O(d^2n)$ ➡ $O(d^2b)$ (※ n:데이터 양, $O()$: 연산량)</ul></ul></ul><hr /><h4 id="미니배치-연산"><strong>미니배치 연산</strong></h4><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-05-57-37.png" alt="p21" /></p><h5 id="기존-경사하강법"><strong>기존 경사하강법</strong></h5><ul><li>경사하강법은 <strong>전체 데이터</strong> $\mathcal{D}=(X, y)$를 가지고 <strong>목적식</strong>($\mathcal{L}$)의 <strong>그레디언트 벡터인</strong> $\large{ \triangledown_\theta\mathcal{L}(\mathcal{D}, \theta) }$를 계산<ul><li>$\triangledown_\theta$ : gradient<li>$\mathcal{L}$ : 목적식<li>$\mathcal{D}, \theta$ : 나블라, 세타<li>$\mathcal{L}(\mathcal{D}, \theta)$ : 전체 데이터 $\mathcal{D}$와 파라미터 $\theta$로 측정된 목적식<ul><li>주어진 파라미터 $\theta$에서 주어진 목적값에 최소점으로 향하는 방향 안내</ul></ul></ul><h5 id="sgd"><strong>SGD</strong></h5><ul><li>SGD는 <strong>미니배치</strong> $\mathcal{D}<em>{(b)} = (X</em>{(b)}, y_{(b)}) \subset \mathcal{D}$를 가지고 그레디언트 벡터를 계산<li>미니배치는 전체 데이터를 사용하는 결과와 같지는 않겠지만 유사하게 나올것임<li>미니배치 $\mathcal{D}_{(b)}$를 가지고 목적식의 <strong>그레디언트를 근사해서 계산</strong><li>SGD는 mini batch를 사용하여 <strong>매번 mini batch samling 할때마다 목적식의 모양이 변화</strong>함<ul><li>매번 다른 미니배치를 사용하기 때문에 곡선 모양이 바뀌게 됨<li>목적식이 조금 바뀌지만 방향은 옳은 방향일 것이라 추측<li>(↔ 경사하강법은 t step을 순차적으로 진행)</ul><li>극소,극대점에서의 gradient vector가 0 벡터가 되더라도, 실제로는 함수 전체의 극소/극대점이 아닐 수 있음 ➡ <strong>기울기를 변형하여 local minima 탈출 가능</strong><ul><li>local minimum 문제를 어느정도 해소할 수 있음(기울기가 0 벡터인 경우의 목적식이 변경되므로!)<li>이 특징을 이용하여 non-convex function에서도 최대/최소점을 찾을 수 있음</ul><li>SGD는 볼록이 아닌 목적식에서도 사용가능하므로 경사하강법보다 머신러닝 학습에 더 효율적<li>학습률, 학습횟수, 미니배치 사이즈 고려 필요<ul><li>너무 작은 미니배치는 일반 경사하강법보다 느린 수렴을 일으킴 <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/sources/2021-01-27-06-07-08.png" alt="p24" /></ul><li>SGD는 일직선으로 움직이지는 않지만, 최소점으로 향하는 움직임을 보이는 성질은 경사하강법과 동일<li>돌아서 가는 것처럼 보이지만 실제로는 더 적은 데이터를 계산하는 것이므로, 움직임(화살표)의 속도가 더 빠름 ➡ 일반 경사하강법보다 빠름! <em>(그림3-수렴속도 비교 확인)</em><li>만일 일반적인 경사하강법처럼 모든 데이터를 업로드하면 메모리가 부족하여 out-of-memory가 발생<li>반면에 미니배치로 쪼갠 데이터를 사용하면 빠르게 연산이 가능하며 하드웨어적인 메모리 부족 한계를 해결할 수 있음<li><del>GPU에서 행렬연산과 모델 파라미터를 업데이트하는 동안 CPU는 전처리와 GPU에서 업로드할 데이터를 준비함</del><li>알고리즘의 효율성과 하드웨어를 고려하면 SGD는 필수!</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech/'>네이버 부스트캠프 AI Tech</a>, <a href='/categories/math-ai/'>Math_AI</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/math/" class="post-tag no-text-decoration" >Math</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛) - Always Awake Sally&url=https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛) - Always Awake Sally&u=https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(매운맛) - Always Awake Sally&url=https://bsm8734.github.io/posts/bc-d007-2-aimath-gradient-descent-hard/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/bc-stage1-d002-lecture/">[부스트캠프 AI Tech / P Stage1] Lecture2 - Dataset & Data Generation</a><li><a href="/posts/bc-stage1-d002-TIL/">[부스트캠프 AI Tech / P Stage1] Day2 TIL</a><li><a href="/posts/bc-d002-5-python-condition-loop/">[부스트캠프 AI Tech / Day2] 파이썬 조건문, 반복문</a><li><a href="/posts/bc-d002-6-python-string/">[부스트캠프 AI Tech / Day2] 파이썬 String</a><li><a href="/posts/bc-d002-today/">[부스트캠프 AI Tech / Day2] Today</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/today/">today</a> <a class="post-tag" href="/tags/dl/">DL</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/imageclassification/">ImageClassification</a> <a class="post-tag" href="/tags/graph/">Graph</a> <a class="post-tag" href="/tags/%EA%B5%AC%EA%B8%80-%EC%8A%A4%ED%84%B0%EB%94%94%EC%9E%BC/">구글 스터디잼</a> <a class="post-tag" href="/tags/nlp/">NLP</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/bc-d006-2-aimath-vector/"><div class="card-body"> <span class="timeago small" > Jan 25 <i class="unloaded">2021-01-25T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[부스트캠프 AI Tech / Day6] Math_AI 벡터</h3><div class="text-muted small"><p> [DAY 6] Vector 벡터 벡터는 공간에서의 한 점을 나타냄 원점으로부터의 상대적 위치를 표현 숫자를 원소로 가지는 리스트(list)/배열(array) 행벡터: 1행으로 이루어진 벡터 $X^T$ 열백터: 1열로만 이루어진 벡터 $X$ 벡터의 차원: 열벡터와 행벡터에서 가장 큰 행, 열의 번호와...</p></div></div></a></div><div class="card"> <a href="/posts/bc-d006-3-aimath-matrix/"><div class="card-body"> <span class="timeago small" > Jan 25 <i class="unloaded">2021-01-25T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[부스트캠프 AI Tech / Day6] Math_AI 행렬</h3><div class="text-muted small"><p> [DAY 6] 행렬 행렬 행렬(matrix)은 벡터를 원소로 가지는 2차원 배열 np.array([[ ],[ ],[ ]]) : numpy에서는 행(row)이 기본단위 세개의 행 벡터를 담는 이차원 배열로 해석됨 행렬은 행(row)와 열(column)이라는 인덱스(index)를 가짐 행렬표기: $...</p></div></div></a></div><div class="card"> <a href="/posts/bc-d007-1-aimath-gradient-descent-easy/"><div class="card-body"> <span class="timeago small" > Jan 26 <i class="unloaded">2021-01-26T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(순한맛)</h3><div class="text-muted small"><p> [DAY 7] Gradient Descent - Easy 미분 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구 최적화에서 가장 많이 사용 미분(differentiation): 변화율의 극한(limit)으로 정의 $\large{ f^\prime(x)=\lim_{h \to 0} { { f(x+h)-f(x) ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/bc-d007-1-aimath-gradient-descent-easy/" class="btn btn-outline-primary"><p>[부스트캠프 AI Tech / Day7] Math_AI 경사하강법(순한맛)</p></a> <a href="/posts/bc-d007-today/" class="btn btn-outline-primary"><p>[부스트캠프 AI Tech / Day7] Today</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://github.com/username">BaeSoomin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/today/">today</a> <a class="post-tag" href="/tags/dl/">DL</a> <a class="post-tag" href="/tags/ml/">ML</a> <a class="post-tag" href="/tags/math/">Math</a> <a class="post-tag" href="/tags/cv/">CV</a> <a class="post-tag" href="/tags/imageclassification/">ImageClassification</a> <a class="post-tag" href="/tags/graph/">Graph</a> <a class="post-tag" href="/tags/%EA%B5%AC%EA%B8%80-%EC%8A%A4%ED%84%B0%EB%94%94%EC%9E%BC/">구글 스터디잼</a> <a class="post-tag" href="/tags/nlp/">NLP</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bsm8734.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
