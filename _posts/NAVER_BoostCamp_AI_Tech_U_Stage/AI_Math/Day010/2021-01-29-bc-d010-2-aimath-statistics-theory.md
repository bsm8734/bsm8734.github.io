---
title: "[부스트캠프 AI Tech / Day10] Math_AI 통계학"
data: 2021-01-29 20:20:00 +0800
categories: [네이버 부스트캠프 AI Tech, Math_AI]
tags: [Math]
use_math: True
---


## **[DAY 10] 통계학**

---

### **모수**

- **통계적 모델링**: 적절한 가정 위에서 확률분포를 추정하는 것
  - 기계학습과 통계학의 공통적 목표
- 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸 다는 것은 불가능하므로  
    ➡ **근사적으로 확률분포를 추정**
    > 예측모형의 목적은 분포를 정확하게 맞추는 것보다는 데이터와 추정방법의 불확실성을 고려해서 위험을 최소화하는 것 
- **모수적(parametric) 방법론**
  - <u>데이터가 특정 확률분포를 따른다고 선험적으로(apriori) 가정한 후, 그 분포를 결정하는 모수(parameter)를 추정하는 방법</u>을
- **비모수(nonparametric) 방법론**
  - 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 방법
    > 기계학습의 많은 방법론은 비모수방법론

### **확률분포**

확률분포를 가정하는 방법: 우선, 히스토그램을 통해 모양을 관찰  

- 데이터가 2개의값(0또는1)만 가지는 경우 ➡ 베르누이분포
- 데이터가 $n$개의 이산적인 값을 가지는 경우 ➡ 카테고리분포
- 데이터가 $[0,1]$사이에서 값을 가지는 경우 ➡ 베타분포
- 데이터가 0이상의 값을 가지는 경우 ➡ 감마분포, 로그정규분포 등
- 데이터가 $\mathbb{R}$(실수) 전체에서 값을 가지는 경우 ➡ 정규분포, 라플라스분포 등

### **모수추정**

- 데이터의 확률분포를 가정했다면, 모수를 추정할 수 있음
- 정규분포의 모수는 평균 $\mu$과 분산 $\sigma^2$으로 이를 추정하는 통계량(statistic)은 다음과 같음
    ![7](/assets/img/sources/2021-01-30-04-48-58.png)
    > 표본분산을 구할 때, $N$이 아니라 $N - 1$로 나누는 이유는 **불편(unbiased)추정량**을 구하기 위해서

- 통계량의 확률분포를 표집분포(sampling distribution)라 부름
- 표본평균의 표집분포는 $N$이 커질수록 정규분포 $\mathcal{N}(\mu, \sigma^2 / N)$를 따름
    > 이를 **중심극한정리(Central Limit Theorem**)이라 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립
- <img src = "/assets/img/sources/2021-01-30-04-52-04.png" width="300px">  

  - 이항분포 형태에서, $N$이 커질수록 정규분포로 변함

### **최대가능도(Maximum Likelihood Estimation, MLE)**

- 표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 됨
- 이론적으로 **가장 가능성이 높은 모수를 추정하는 방법** 중 하나
    ![10](/assets/img/sources/2021-01-30-05-03-50.png)

> 가능도(likelihood)함수는 모수 $\theta$를 따르는 분포가 $x$를 관찰할 가능성을 뜻하지만 **확률로 해석하면 안됨**

- 데이터집합 $X$가 독립적으로 추출되었을 경우 **로그가능도를 최적화**
    ![11](/assets/img/sources/2021-01-30-05-05-14.png)

### **로그가능도**

- 로그가능도 사용하는 이유
  - 로그가능도를 최적화하는 모수 $\theta$는 가능도를 최적화하는 MLE가 됨
- 데이터의 숫자가 적으면 상관없지만 만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능
- 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해짐
- 경사하강법으로 가능도를 최적화 할 때, 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 연산량을 $O(n^2)$에서 $O(n)$으로 줄여줌
- 대게의 손실함수의 경우, 경사하강법을 사용하므로 음의 로그가능도(negativelog-likelihood)를 최적화하게 됨

### **최대가능도 추정 예제**

![13](/assets/img/sources/2021-01-30-05-09-16.png)

#### **정규분포 예제**

![16](/assets/img/sources/2021-01-30-05-19-35.png)

---

#### **카테고리분포 예제**

![22](/assets/img/sources/2021-01-30-05-29-36.png)

---

### **확률분포 사이의 거리**

- 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도
- 데이터 공간에 두 개의 확률분포 $P(x), Q(x)$가 있을 경우, 두 확률분포 사이의 거리(distance)를 계산할 때 다음과 같은 함수들을 이용
  - 총변동 거리(Total Variation Distance, TV)
  - 쿨백-라이블러발산(Kullback-Leibler Divergence, KL)
  - 바슈타인거리(Wasserstein Distance)

#### **쿨백-라이블러발산(KL Divergence)**

![25-1](/assets/img/sources/2021-01-30-05-37-16.png)  
이는 다음과 같이 분해할 수 있음  
![25-2](/assets/img/sources/2021-01-30-05-38-05.png)

- 분류문제에서 정답레이블을 $P$, 모델예측을 $Q$라 두면, 최대가능도추정법은 쿨백-라이블러 발산을 최소화하는 것과 같음

#### **딥러닝에서의 최대가능도 추정법**

- 최대 가능도 추정법(MLE)을 이용해서 기계학습 모델을 학습할 수 있음
- 딥러닝 모델의 가중치를 $\theta = (W^{(1), ... , W^{(L)}})$라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리 분포의 모수 $(p_1, ..., p_k)$를 모델링
- 원-핫 벡터로 표현한 정답레이블 $y = (y_1, ..., y_k)$을 관찰데이터로 이용해, 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있음
    ![23](/assets/img/sources/2021-01-30-05-34-11.png)
