---
title: "[부스트캠프 AI Tech / Day21] Today"
data: 2021-02-22 22:31:00 +0900
categories: [네이버 부스트캠프 AI Tech, Today]
tags: [today]
---


## **피어세션 정리**

- `후미`
  - 다운스트림 태스크가 뭔지 궁금
  - 파인 튜닝 할 때 새로운 부분을 찾기?

- `엠제이`
  - BERT 에서 트랜스포머 디코더 부분 없나요?
    - 인코더로 이루어짐
    - 참고자료 https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/ 
  - 전이학습과 파인 튜닝의 차이
    - https://choice-life.tistory.com/m/40
    - 전이 학습 : 데이터 부족해서 데이터 풍부한 학습된 모델 사용하는 것
    - 파인 튜닝 : 기존 학습 모델 기반으로 목적에 맞게 변형 (마지막 단 조정)

- `히스`
  - 실습의 균일 분포, 실제 분포, 랜덤 분포에서 지름과 전역 군집 계수 이해안감
  - https://github.com/boostcamp-ai-tech-4/peer-session/issues/83

### 지난주에 해결 안된 질문들

- 마스크 부분이 버트를 통과하면 답으로 채워지는데, 어떤 식으로 되는지 궁금함
  - 12개 인코더 거쳐서 단어간의 관계를 잡아주는데 마지막에 나온 결과와 정답을 비교해서 백워드하면서 관계를 잘 잡아주도록 하지 않을까
- 어떤 식으로 self-supervised 러닝이 되는건지 궁금
  - 원래 문장은 어떤 라벨 같은게 없는데 단어를 마스크 시켜서 원래 단어 찾아내도록 하는거랑 문장 연결시켜서 다음 문장을 예측하도록 만들어서 라벨링이 있는것 처럼 학습
  - 덕분에 비지도 학습이 아니라 지도학습이라고 하는 의견도 존재하는것으로 앎. 
  - https://greeksharifa.github.io/self-supervised%20learning/2020/11/01/Self-Supervised-Learning/
  - https://velog.io/@tobigs-gm1/Self-Supervised-Learning
- BERT의 Masked Language Model의 단점은 무엇이 있을까요? 사람이 실제로 언어를 배우는 방식과의 차이를 생각해보며 떠올려봅시다.
  - 마스크가 되면서 독립성을 띄어 마스크간의 관계를 구할 수 없음
  - 또한 마스크를 사용하여 pretrain한것이 fine tuning이랑 학습데이터가 다르기 때문에 어느정도 손해를 봄

---

## **과제 진행 상황**

- 과제 현황
  - [X] 과제 1-1 (인접리스트, 인접행렬로 구현한 그래프 함수 채우기)

---

## **과제 결과물에 대한 정리**

- 그래프의 구현방법에 대해서 익혔다.
- C++을 주로 사용하면서, 구현도 C++로 밖에 안해봤는데, Python으로 무언가 구현하는 방식과는 좀 차이가 있는 것 같다.

---

## **TIL (Today I Learned)**

- 그래프에 대한 심화된 이해를 하게 되었다.
- fine tuning과 transfer learning를 그동안 비교해보지 않았던 것에 충격! 🔎 이걸 왜 지금에서야 생각했지!

---

## **오늘의 한마디**

- 지친 저번주는 떨쳐내고, 이번주부터는 다시 힘내서 달려봅시당!
