generative model part 2

lecture 10

저번: generative model이 뭔지
implic, explic, arm이 뭔지

2
latent var model

3
auto encoder는 gm 인가?
그렇지 않다
어떻게 variation auto encoder가 gm이 될 수 있는지?

4
Variational Auto-encoder
Variational Inference
thesis읽어보기
posterior distri : 관측값이 주어졌을 때, 관심있는 RV의 확률분포 (likelihood의 반대)
z: latent RV
VI의 목적: 찾고자하는 poster을 잘 근사할 수 있는 분포(variational distibuttion?)를 찾기
무언가를 잘 찾겠다 -> 최적화 중요!
kL divergen사용하겠다

5
목적 P(z|x)
관심있는 posterior 분포를 근사하는 변수를 찾기

6
타깃을 모르는데 최적화? 어떻게?
variational distibuttion & posterior dist의 KL diver(차이)를 줄이겠다
ELBO를 키워서 내가 원하는 것을 얻겠다
ELBO 수식은 exact
어떤 거리를 줄일 수 있는 어떤 목적을 ELBO를 최대화 해서 얻을 수 있음
샌드위치 법칙이라 부를 수도 있음
Vari auto encoder를 최적화 하는 방법

7
ELBO 계산 가능
reconst  term: loss func 
prior term: latent
x에 대한 입력(img의 공간)이 있고 얘를 잘 표현하기 위한 z라는 latent space 를 찾고싶은 것
z라는 latent를 잘 표현하기 ?
poster를 모르니까 
인코더로 얘를 근사하고 싶은것
근데 근사할 수 없으니까
ELBO를 최대화 하는 것이 KL div를 줄여주는 것과 같은 효과를 내더라 하는 것을 유도!
그리고, 이 것은 두가지 항으로 나뉜다
reconstruction loss: encoder통해서 x를 latent space로 보냈다가, decoder로 돌아오는 reconstruction loss를 줄임
prior fitting term: x라는 점들이 많이 있을 때 이것들을 latent space에 올려놨음 -> 점들 -> 점들이 이루는 분포가 사전분포? 두개를 만족하는 것과 같다
그래서 AVE가 생성모델이 될 수 있음
explicit아니고 implic

8

encoder을 통해서 latent space로 보내고, 무언가를 찾고, 다시 reconstruction 하는 term으로 만드는데, 
생성모델로 만드는데, 샘플링하고, 디코더 태워서 나오는 값이 우리의 generation model 으로 볼 수 있음?
그래서 엄밀히 생성모델은 아님
제약
- 얼마나 그럴싸한지 알기 힘듦
- reconstruction은 그냥 하면되는데, kL divergence 집어넣어서 최적화 시키니까, 여기에 있는 적분으로 인해 계산이 힘듦
- 가우시안이 몇 안되는 KL div? 에 있는 분포이기 때문
- isotripic gaussian: 모든 output dim이 독립적
- 아주 예쁜 식으로 나옴
- 그러면 우리가 원하는 결과가 나옴?

그래서 무언가 만드는데 좋은 방법

10 AAE
이전거가 KL div를 사용해야한다는 단점이 있음(가우시안을 따라야만함)
AAE: GAN을 활용해서 분포를 맞추는 것
피팅 항을 GAN으로 바꿈
샘플링 가능한 어떤 분포만 있으면 활용가능
왓설? 머??? distance를 줄이는 것과 같음
generative 성능도 좋음

11 GAN

GAN
가짜를 만드는 모델과 진짜를 구분하는 모델로 구성
generator의 성능을 높이는게 목적
장점: 결과로 나오는 discriminator가 점차 좋아지는것이 장점, generator를 통해서 dis가 잘나오므로 gener도 잘됨
implicit model

13 GAN vs VAE
VAE은 x라는 input domain이 들어오면 encoder를 통해서 z로 갔다가(latent vector) 디코더를 통해서 x라는 도메인으로 가고, 학습을 하고 생성단계에서는 p(z)에 디코더를 써서 판별해 낸 결과를 데려옴
GAN은 분류기를 학습해서 D, G를 번갈아가면서 학습
D를 통해서 진짜와 가짜를 구분하는 분류기를 학습
G는 그렇게 학습한 D의 입장에서 True가 나오도록 다시 학습
D는 이렇게 나온 결과값이 진짜와 구분될 수 있도록 학습

13 GAN objective
minmax game : 한쪽은 높이고 싶어하고, 한쪽은 낮추고 싶어하는

14 D의 입장에서의 식
form을 항상 최적화 시키는 D는 아래의 꼴이 된다
()/()이 optimal discrimina
G가 fix일 때, 얘를 항상 최적으로 갈라주는 optimal discri는 D*라는 ?
얘가 높으면 T, 낮으면 F 이게 optimal discriminator
그러면, 이 optimal discriminator를 다시 Generator 축에 다시 넣으면

15
JSD로 나오게 됨
D와 G의 얀센 거리를 줄이는 것(미지의 distibution과 내가 학습한 G 사이에 거리 줄이기)
GAN objective가 많은 경우를 데이터를 실제로 만들었다고 만들었다고 생각하는분포와 실제의 분포를 JSD를 최소화 했다고 함

 D가 optial이라고 가정 했을 때,optimal D를 G입장에서 봤을 때, 학습하는거에 집어넣고 봤더니 15페이지의 식을 만족

JSD가 optimal disctiminator에 수렴한다는 것도 보장하기 힘들고, 그렇게 했을 때, G가 이렇게 안나올수도 있음
그래서 이론적으로 되는 말이지만, 현실적으로 JSD를 줄인다고 했을 떄 의아할 수잇음

동일한 논리는 AAE를 ~ 인코더로 해석할 때 활용?

JSD : GAN objective은 True Generative distribution과 내가 학습하고자하는 generator 사이의 JSD를 최소화하는 것이라는 식

16 DCGAN
첫 GAN :MLP(Dense)로 만들었움
DCGAN: image domain이다
image domain에서 벡터를 늘리기 위해서 **deconv**활용
generator에는 deconvol layer
discriminator에는 conv
알고리즘 적으로 좋은 점은 없으나 좋은 테크닉은 알려줌

17 Info-GAN
이미지를 만들때 단순히 G라는 것을 통해서 이미지를 만드는 것이 아니라 C라는 ancillary class를 랜덤하게 매번 집어넣는 것
그럼 얘를 랜덤한 원핫벡터라고 할 수 있겠다
> 1 : of lower or secondary class or rank : subordinate, subsidiary. 2 : providing additional help or support : auxiliary, supplementary.
> Ancillary | Definition of Ancillary by Merriam-Webster

-> 결과적으로 생성할 때, GAN이 특정 모드에 집중할 수 있게 해줌
c라는 모드에 집중할 수 있게 해줌
마치 multi modal distribution를 학습하는것을 c라는 벡터로 잡아주는 것과같음

18
문장을 이미지로 만들어줌
input이 문장이고 조건에따라...

19
Puzzle GAN
이미지 안에 서브패치가 있음 - 각각의 구성요소
그 구성요소를 각각 복원

20
갠을 활용하지만 이미지 사이에 도메인을 바꿀 수 있음
cycle consistence가 매우매우 중요
원래) 말을 얼룩말로 변경 -> 두개의 똑같은 이미지에 말과 얼룩말이 필요
그러나 얘는 그렇게 할 필요없음
그냥 많은 말과 많은 얼룩말이 있으면 됨
두개의 GAN

22
모드를 정해줄 수 있음
성, 나이, 창백, ...
이미지를 단순히 다른 도메인으로 바꾸는게 아니라 control 할 수 있게
많은 practical 문제에 사용됨
굉장히 실제적인 이미지 만들 수 있음

23
고차원의 이미지를 잘 만들 수 있는 이미지
되게 좋은 성능
progressive하게
44 -> 1024 1024
저차원부터 고차원으로 점차 늘려가게 됨 (한번에 X)
굉장히 좋은 성능으로 만들 수 있게 됨

---

이 강의는 practical하게....
VAE에서 VI만들어진 이유
실제 학습 목적
GAN objective
GAN목적 얀센거리..줄이기...

