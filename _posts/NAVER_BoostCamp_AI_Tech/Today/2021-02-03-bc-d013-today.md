---
title: "[부스트캠프 AI Tech / Day13] Today"
date: 2020-02-03 22:31:00 +0900
categories: [네이버 부스트캠프 AI Tech, Today]
tags: [today]
---


## **피어세션 정리**

- 오늘은 `제리`(조교)님이랑 함께 피어세션을 진행!
- `엠제이`: colab 연결 이슈
- `엠제이`: CNN 모델에서 convolution layer 에서 dense로 넘어 갈때 파라미터가 급등하는 이유
  - convolution layer에서는 커널 사이즈에 따라 파라미터 수가 결정이 되는데 dense(Fully Connected)로 넘어 갈때는 tensor 사이즈가 결정합니다.
  - kerner 크기= K , channel =C , kernel의 개수 =N
  - conv 연산 = K x K x C x N
  - conv layer 크기 = I , channel = C , output FC layer 크기(뉴런 수) = O
  - **FC 연산 = I x I x C x O**
- `샐리`: 채널 개수는 어떻게 처리?
  - 채널은 더하는 게 아니라 예를 들어, (ch, h, w) = (3, 3, 3) 27개를 일렬로 나열하는 것

> 그럼 flatten한 결과는 (1, 27)이 나옴. 근데 최종 label이 10이라고 할 때, (1, 27) ➡ (1, 10)으로 변환을 하려면 (27, 10) 크기의 weight 행렬이 필요!
> 그래서 최종적으로 27*10의 파라미터 개수가 필요하게 됨
> ➡ feature map을 flatten하기 때문에 파라미터의 개수가 많이 늘어나게 됨

- `샐리` & `후미` & `엠제이`: ResNet 이 잘 이해가 안됨
  - Deeper neural networks are hard to train
  - ➡ 기존은 DNN 은 기울기 소실(vanishing gradient) 문제 때문에 layer를 깊게 쌓지 못했지만 ResNet은 Residual Block을 사용하면서 문제 해결
- ResNet에서 **차이**만을 학습 한다는게 무슨 뜻인가?
  - ➡ (아래 그림 참고) 보통의 경우에는 conv layer를 통과해 나온 $h(x)$를 바로 학습에 투입하는 반면 ResNet의 Residual(잔차)에서는 출력으로 나온 $h(x) - x$를 학습에 투입
  ![image](https://user-images.githubusercontent.com/26226101/106730612-ea0c7080-6651-11eb-8aae-9962a98090a7.png)
- 이 잔차를 이용해 기울기 소실(vanishing gradient)을 해결하는 것인가?
  ➡ ⭕ 맞다고 할 수 있음. 실제로 ResNet이 나온 이후 네트워크 레이어들이 깊어 졌음
- `히스`: CNN 레이어가 깊을 때 각 커널의 역할
  - [링크](https://github.com/boostcamp-ai-tech-4/peer-session/issues/53)
- `히스`:  Gradient Vanishing 해결을 위한 ReLU 와 ResNet
  - [링크](https://github.com/boostcamp-ai-tech-4/peer-session/issues/52)


##### ETC) 더 정리할 것

> ▪ **캡슐넷**
> ➡ 각각 conv 필터들이 각각의 특징을 가지고 있다
> 이 필터는 어떤 특징을 추출/구별하는 필터인지 정해져있다

> ▪ **translation invariant 이동 불변성**
> "좌측에 있던, 우측에 있던, 고양이는 고양이이다"

> ▪ **lotation invariant: 크기**
> 특징에 대한 불변성에 대해서 더 알아보기

> ▪ **1x1은 항상 좋을까?**
> google net inception module
> ➡ 수많은 정보를 하나로 압축 3x3x3이 하나의 정보로 압축되면 손실이 있을 것이다?

---

## **과제 진행 상황**

- 과제 현황
  - [ ] CNN assignment

---

## **과제 결과물에 대한 정리**

- 추후 보완 필요

---

## **TIL (Today I Learned)**

- 오늘의 구글링 팁
  - 한글로 찾지말것 (특히 ML 관련), 영어로 검색해 볼 것!
  - 영어 문서가 이해 안가면 '구글' 번역기 활용(전문용어에 대한 해석을 좀더 의미있게 해줌)
  - 영어로 검색하고 한국어웹 필터 적용해서 보기

---

## **오늘의 한마디**

- 밀리지 않으려고 노력하는 스스로가 대단!
- 1일 1커밋, 잔디 채워지는 모습이 뿌듯!
